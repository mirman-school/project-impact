{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distribution Part 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/mirman-school/project-impact/blob/master/Distribution_Part_1.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "cWyufaryCdvk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Distribution, Part I\n",
        "\n",
        "Let's take a step back from pandas for a second and talk a little bit about the kinds of numbers we're pulling from DataFrames, and the mathematical tools we'll use to draw conclusions.\n",
        "\n",
        "In particular, we'd like to talk about **distribution** and **central tendency**.\n",
        "\n",
        "Let's start by loading up some Python modules we'll need to work with."
      ]
    },
    {
      "metadata": {
        "id": "4r0xXk9rHdmI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Central Tendency\n",
        "\n",
        "Numerical values exist in a range. There's a highest, a lowest, a midpoint, and everything in between. Learning how our values are arranged can help us understand the story of the data.\n",
        "\n",
        "Let's start by making a dataset. We're going to make a set of test scores. We'll use `numpy` to create a random (but not-so-random) set of scores to start with."
      ]
    },
    {
      "metadata": {
        "id": "sUp_W0iEg4Yp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np # Easy access to common mathematical functions\n",
        "import matplotlib.pyplot as plt # Graphing\n",
        "import random # Making random stuff\n",
        "import scipy.stats as stats # Extra statistical functions\n",
        "import math # ...math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o-DTvCoWhn-2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "min_test_score = 50 # The low end of our test score range\n",
        "max_test_score = 100 # The high end\n",
        "total_test_scores = 20 # How many tests to generate\n",
        "\n",
        "np.random.seed(42) # Guarantee the same output for everyone\n",
        "test_scores = np.random.randint(min_test_score, max_test_score + 1, total_test_scores) # + 1 on max because randint is top-end-exclusive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ydbnFwKUickX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now we need to sort our dataset. Why?\n",
        "\n",
        "2 of the 3 of our basic analysis tools (mean, median, and mode) require the data to be _sorted_ to work. **Median**, or the midpoint of the data, only makes sense if the values are in order. Similarly, **mode** is easiest to discern when like values are grouped, making it easy to count how many occurrences of each value there are."
      ]
    },
    {
      "metadata": {
        "id": "W1yh1qy3iHoa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sorted_scores = sorted(test_scores)\n",
        "sorted_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PNegZHJmZQKM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mean $$\\bar{x}\\\\ \\\\$$\n",
        "\n",
        " \n",
        " $$\\bar{x} = \\frac{1}{n}\\left (\\sum_{i=1}^n{x_i}\\right ) = \\frac{x_1+x_2+\\cdots +x_n}{n}$$\n",
        " \n",
        " **Mean** is, as you can _clearly_ see above, is defined as the sum of the values  in a set divided by the number of values in that set. It is the same as the arithmetic mean. \n",
        " \n",
        "We could write a loop and do some simple Python math to calculate this value. But we're hackers now, so here's a fun hack to use: `np.mean()` will take a list or numpy array and return the arithmetic mean.\n",
        "\n",
        "Use `np.mean()` below to get the average from `test_scores`."
      ]
    },
    {
      "metadata": {
        "id": "Bq11o2-PaQaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use np.mean() to get the average from test scores\n",
        "tests_mean = None\n",
        "# Print it out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hwz4ZLq3adtu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Median $$\\tilde{x}$$\n",
        "\n",
        "A dataset's **median** is the midpoint of the data. When sorted, it's the position where there are as many values above as below. Now you see why we might need `sorted_scores`!\n",
        "\n",
        "Haha, j/k, there's `np.median()` so you don't need to worry about that too much.\n",
        "\n",
        "Note that if the median falls between two values in the set, the median is defined by the **mean** (see above) of those two values.\n",
        "\n",
        "Use `np.median()` to get the median from `test_scores`."
      ]
    },
    {
      "metadata": {
        "id": "UB6aw86gbLJY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use np.median() on test_scores\n",
        "tests_median = None\n",
        "\n",
        "# Print it out\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jw-HPNh6NhLO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mode\n",
        "\n",
        "**Mode** is defined as the most commonly occurring value in a set. While we could easily count up occurrences of values in a small set, we want a solution that scales to big datasets. Luckily, `scipy.stats` has us covered with a built-in `mode()` function, used like:\n",
        "\n",
        "```python\n",
        "stats.mode(my_dataset)\n",
        "```\n",
        "\n",
        "But what it returns might look kinda weird. Let's check it out."
      ]
    },
    {
      "metadata": {
        "id": "zAgi1czEPJSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tests_mode = stats.mode(test_scores)\n",
        "tests_mode"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hsfAHE72RcGD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "What in the jam is a `ModeResult`?  It's a custom data type from `scipy` that includes both the (smallest) mode from a dataset, and the count of occurrences of that mode.\n",
        "\n",
        "Why are they still inside arrays? Because `stats.mode()` can handle numpy arrays of multiple dimensions. Ours is 1-d, but we could imagine an array of 2 or more dimensions. `stats.mode()` would find the mode in each of those sub-arrays.\n",
        "\n",
        "`ModeResult`s have 2 properties: a `mode` that contains an array of all the modes found in the dataset, and a corresponding `count` array that indicates the number of occurrences of the modes, in order. So the `ModeResult` above shows that there was 1 mode, 73, and that it occurred 3 times in the dataset. \n",
        "\n",
        "To just access the `mode` array, use [**dot notation**](http://reeborg.ca/docs/oop_py_en/oop.html). \n",
        "\n",
        "We'll worry about that later. For now, let's get the mode from `test_scores`."
      ]
    },
    {
      "metadata": {
        "id": "t0S2U54KA55y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use dot notation to get just the mode array from test_mode\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VRCDqxDebn77",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Distribution\n",
        "\n",
        "To understand how our data plays out across min, max, and everything in between, it'll be helpful to create a distribution plot, or **histogram**. \n",
        "\n",
        "A histogram is a special kind of bar chart. On the x-axis is the range of values. On the y-axis, the count of how many of each value occurs in the dataset.\n",
        "\n",
        "Here's how we make one."
      ]
    },
    {
      "metadata": {
        "id": "WFpmMI3xHCPO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.hist(test_scores)\n",
        "plt.title(\"Test score distribution (10 bins)\")\n",
        "plt.xlabel(\"Test scores\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l218FhIlHRVF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Bins\n",
        "\n",
        "Our `test_scores` dataset is unique in that the value fall within a small and predictable range of integers. It wouldn't be hard to read to have a bar for each possible value. In larger datasets, it makes more sense to group values together into **bins**. Using `pyplot`, the default number of bins is 10.  Changing the number of bins can change the story the histogram tells. You can change the number of bins by passing the optional `bins` parameter to `plt.hist()`."
      ]
    },
    {
      "metadata": {
        "id": "wlkP4Y9cHtfo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.hist(test_scores, bins=20) # Change this to see how the graph changes\n",
        "plt.title(\"Test score distribution (20 bins)\") # Change the title too!\n",
        "plt.xlabel(\"Test scores\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DAfO_47GKzG0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Comparing Datasets\n",
        "\n",
        "We have our tiny dataset of 20 test scores. What would happen with 200 test scores? 2000? 20000? Our random number generator, by default, should be completely fair, meaning that there's equal probability of retrieving any number in the given range.\n",
        "\n",
        "Sooooo, what _should_ happen as our datasets get bigger, is that the histogram should get flatter and flatter.\n",
        "\n",
        "Let's find out."
      ]
    },
    {
      "metadata": {
        "id": "Up3J0m5PLqBK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll start by generating 4 new datasets using `np.random.randint()`. We need 4 sets, of **50**, **500**,  **5000**, and **50000**. Make them as above with `np.random.randint()`."
      ]
    },
    {
      "metadata": {
        "id": "yg1em6MgRUsL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Replace None with the correct usage of np.random.randint()\n",
        "small = np.random.randint(50,101, 50)\n",
        "medium = np.random.randint(50,101, 500)\n",
        "large = np.random.randint(50,101, 5000)\n",
        "xlarge = np.random.randint(50,101, 50000)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ri7yXtjqUIHx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Comparing by overlay\n",
        "\n",
        "One way to compare our datasets is by overlaying the plots on the same grid. Obviously the large dataset is going to have much higher counts than the small, but we should still be able to see the shape of the distribution.\n",
        "\n",
        "All we need to do to overlay our histograms is repeatedly call `plt.hist()` with each dataset in turn.\n",
        "\n",
        "However, to make all of the hists visible, we need to make all but the first histogram a little see-through. We should also change their color to make them clearly different, like so:\n",
        "\n",
        "```python\n",
        "# Makes a histogram with 20 bins, red, and 40% opacity\n",
        "plt.hist(dataset, bins=20, color=\"r\", alpha=0.4)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "cvAQEdXZbxLv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[20,10])\n",
        "plt.hist(small, bins=51)\n",
        "plt.hist(large, bins=51, color=\"r\", alpha=0.4)\n",
        "plt.hist(medium, bins=51, color=\"g\", alpha=0.3)\n",
        "plt.hist(xlarge, bins=51, color=\"purple\", alpha=0.3)\n",
        "plt.title(\"Test score distribution\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4swYQ1x-ZihD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### PDF\n",
        "\n",
        "Right, so that's kinda hard to read. With 50k in the `xlarge` dataset,  everything else is going to look a little squished. We can normalize the sizes by using a [probability density function](https://en.wikipedia.org/wiki/Probability_density_function).\n",
        "\n",
        "A what?\n",
        "\n",
        "A proability density function takes a dataset and creates a distribution that shows not raw counts, but the _probability_ that a given value will be at that particular position in the range. PDFs return values between 0 and 1, regardless of the size of the dataset. What that means is that we can look at the PDF for each dataset without squishing any. \n",
        "\n",
        "It's pretty easy to use PDFs with `plt.hist()`. It's just a parameter you pass, like `color` or `alpha`. Set `density` to `True` for each of your histograms. Otherwise, reuse the code from above."
      ]
    },
    {
      "metadata": {
        "id": "rulK78Gpl7D1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Use the code from above to remake your histograms, but use density set to True for each hist\n",
        "plt.figure(figsize=[20,10])\n",
        "plt.hist(small, bins=51, density=True)\n",
        "plt.hist(large, bins=51, color=\"r\", alpha=0.4, density=True)\n",
        "plt.hist(medium, bins=51, color=\"g\", alpha=0.3, density=True)\n",
        "plt.hist(xlarge, bins=51, color=\"purple\", alpha=0.3, density=True)\n",
        "plt.title(\"Test score distribution\")\n",
        "plt.xlabel(\"Score\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JbEIvkZjnPBg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Subplot\n",
        "\n",
        "Yeah, so that works, except that now it's hard to tell one plot from another. So instead, we're going to split our plot into 4 pieces and look at the plots side-by-side.\n",
        "\n",
        "`plt.subplot()` takes 3 arguments: a row, a column, and a cell number. Once you call `plt.sublplot()`,  further `plt` functions will apply to that subdivision of the whole figure. So for example.\n",
        "\n",
        "```python\n",
        "\n",
        "plt.subplot(1,1,1) # Row 1, Column 1, Cell 1 (top left)\n",
        "... # some stuff here\n",
        "plt.subplot(1,2,2) # Row 1, Column 2, Cell 2 (top right)\n",
        "... # some stuff here\n",
        "plt.subplot(2,1,3) # Row 2, Column 1, Cell 3 (bottom left)\n",
        "... # some stuff here\n",
        "plt.subplot(2,2,4) # Row 2, Column 2, Cell 4 (bottom right)\n",
        "```\n",
        "\n",
        "So what we want to do is call `plt.subplot()`,  then do our work for one single histogram (including title and labels), then call `plt.subplot()` again and repeat the process.\n",
        "\n",
        "Using `plt.subplot()`, create 4 plots—one for each dataset."
      ]
    },
    {
      "metadata": {
        "id": "td66dRawpiCV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reproduce the code above, but 4 times, using plt.subplot() between to create 4 individual plots\n",
        "# One for each dataset: small, medium, large, and xlarge\n",
        "\n",
        "plt.figure(figsize=[20,10])\n",
        "\n",
        "# use subplot to select the right part of the graph\n",
        "\n",
        "# create the histogram\n",
        "\n",
        "# add title, xlabel, and ylabel\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QtCN8hzSuqDd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If everything's gone to plan, you'll see that the `xlarge` distribution gets pretty even. This is what's known as a _regular_ distribution."
      ]
    },
    {
      "metadata": {
        "id": "3-k7PiWGNsBS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Adding Mean/Median/Mode to the plot\n",
        "\n",
        "Remember the central tendency tools we used earlier? It can be handy to add them as vertical lines to our histograms. `plt.axvline()` can help us out here. We just give it the x-value, and the color we want, and bam! A line on the plot. Let's use the `test_scores` dataset as an example."
      ]
    },
    {
      "metadata": {
        "id": "-aGAMEKVSIOQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=[20,10])\n",
        "plt.hist(xlarge, bins=51, density=True)\n",
        "plt.axvline(np.mean(xlarge), color=\"r\", linewidth=3)\n",
        "plt.axvline(np.median(xlarge), color=\"g\", linewidth=3)\n",
        "plt.axvline(stats.mode(xlarge).mode[0], color=\"purple\", linewidth=3)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jccEIRf2sKuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## END OF PART 1\n",
        "\n",
        "Congratulations! You made it! In the next notebook, we'll cover variance in data, and how to use it."
      ]
    }
  ]
}